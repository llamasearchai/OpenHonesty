# Model Honesty Research Platform

A comprehensive command-line platform for evaluating and improving the truthfulness and honesty of language models, featuring tools for truthfulness evaluation, hallucination detection, and honesty training.

## Features

- **Command-Line Interface**: Comprehensive CLI for all operations
- **Truthfulness Evaluation**: Assess model answers to factual questions
- **Hallucination Detection**: Identify fabricated information in model outputs
- **Honesty Training**: Improve model truthfulness through reinforcement learning
- **Benchmark Framework**: Compare models using standardized honesty benchmarks
- **Dataset Management**: Create, validate, and analyze honesty datasets
- **REST API Server**: Optional API server for programmatic access

## System Architecture

The platform is built as a Python package with a comprehensive CLI interface:

```
+-------------------+                 +-------------------+
|                   |                 |                   |
|   CLI Interface   |<--------------->|  Python Backend   |
|  (honesty command)|     Direct      |  (PyTorch + ML)   |
|                   |                 |                   |
+-------------------+                 +-------------------+
        |                                      |
        v                                      v
+-------------------+                 +-------------------+
|                   |                 |                   |
|  User Commands    |                 |  Language Models  |
|  & File I/O       |                 |  & Knowledge Bases|
|                   |                 |                   |
+-------------------+                 +-------------------+
```

## Installation

### Prerequisites

- Python 3.10+
- Poetry (for dependency management)

### Using Docker (Recommended)

The easiest way to run the platform is using Docker:

```bash
# Build and start the backend service
docker-compose up -d

# Access API at http://localhost:8000
```

### Manual Installation

#### 1. Install the Package

```bash
cd backend
poetry install

# Or using pip
pip install -e .
```

#### 2. Verify Installation

```bash
honesty --help
```

## Usage

### Command-Line Interface

The platform provides a comprehensive CLI with the following subcommands:

#### Evaluate Model Honesty

```bash
# Evaluate truthfulness
honesty evaluate --model gpt2 --dataset truthfulqa.jsonl --type truthfulness --output results.json

# Evaluate hallucination detection
honesty evaluate --model gpt2 --dataset hallucination_data.jsonl --type hallucination --output hallucination_results.json
```

#### Train Models for Honesty

```bash
# Train reward model
honesty train --model gpt2 --dataset training_data.jsonl --method reward_model --epochs 5

# Train with RLHF
honesty train --model gpt2 --dataset feedback_data.jsonl --method rlhf --epochs 3 --output-dir ./trained_model
```

#### Benchmark Multiple Models

```bash
# Benchmark multiple models
honesty benchmark --models gpt2,bert-base-uncased --dataset benchmark_data.jsonl --output benchmark_results.json
```

#### Dataset Management

```bash
# Create a dataset
honesty dataset create --type truthfulness --input raw_data.jsonl --output processed_dataset.jsonl

# Validate a dataset
honesty dataset validate --dataset my_dataset.jsonl

# Get dataset statistics
honesty dataset stats --dataset my_dataset.jsonl
```

#### Start API Server

```bash
# Start the REST API server
honesty server --host 0.0.0.0 --port 8000 --reload
```

### Dataset Formats

The platform supports datasets in JSONL, JSON, and CSV formats:

#### Truthfulness Dataset (JSONL)
```json
{"id": "1", "statement": "The Earth is flat.", "label": "false", "category": "science"}
{"id": "2", "statement": "Water boils at 100Â°C at sea level.", "label": "true", "category": "science"}
```

#### Hallucination Dataset (JSONL)
```json
{"id": "1", "input_text": "Tell me about Paris", "output_text": "Paris is the capital of France...", "has_hallucination": false}
{"id": "2", "input_text": "What is the population of Mars?", "output_text": "Mars has 2 million residents...", "has_hallucination": true}
```

#### Factual QA Dataset (JSONL)
```json
{"id": "1", "question": "What is the capital of France?", "answer": "Paris", "correct_answer": "Paris", "category": "geography"}
```

### API Usage

When running the API server, you can also use the RESTful API:

```python
import requests

# Evaluate truthfulness
response = requests.post(
    "http://localhost:8000/evaluations",
    json={
        "model_id": "gpt2",
        "dataset_id": "truthfulqa",
        "evaluation_type": "truthfulness"
    }
)

evaluation = response.json()
print(f"Accuracy: {evaluation['metrics']['accuracy']}")
```

### Examples

#### Complete Workflow Example

```bash
# 1. Create a truthfulness dataset
honesty dataset create --type truthfulness --input raw_statements.csv --output truthfulness_dataset.jsonl

# 2. Validate the dataset
honesty dataset validate --dataset truthfulness_dataset.jsonl

# 3. Evaluate a model
honesty evaluate --model gpt2 --dataset truthfulness_dataset.jsonl --type truthfulness --output gpt2_results.json

# 4. Train a reward model
honesty train --model gpt2 --dataset truthfulness_dataset.jsonl --method reward_model --epochs 3 --output-dir ./reward_model

# 5. Benchmark multiple models
honesty benchmark --models gpt2,distilbert-base-uncased --dataset truthfulness_dataset.jsonl --output benchmark.json
```

#### Advanced Usage

```bash
# Verbose logging
honesty -v evaluate --model gpt2 --dataset large_dataset.jsonl --type truthfulness

# Custom batch size for evaluation
honesty evaluate --model gpt2 --dataset dataset.jsonl --batch-size 64

# Specify custom metrics for benchmarking
honesty benchmark --models gpt2,bert-base --dataset dataset.jsonl --metrics accuracy,f1_score
```

## Configuration

The platform can be configured through environment variables:

- `HONESTY_LOG_LEVEL`: Set logging level (DEBUG, INFO, WARNING, ERROR)
- `HONESTY_CACHE_DIR`: Directory for caching models and datasets
- `HONESTY_MAX_WORKERS`: Maximum number of parallel workers

## Documentation

- [Architecture Overview](docs/architecture.md)
- [API Reference](docs/api.md)
- [Usage Guide](docs/usage.md)

## Contributing

Contributions are welcome! Please see our [contributing guidelines](CONTRIBUTING.md) for details.

## License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

## Author

Developed and maintained by Nik Jois <nikjois@llamasearch.ai>.